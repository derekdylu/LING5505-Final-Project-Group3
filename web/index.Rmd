---
title: "**台灣升大學考試國寫情意題佳作詞頻相關研究**"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: sandstone
---
**第三組 想不到組名**

**組員：朱修平、盧德原、楊舒晴、陳宛瑩**

<br>

<!-- PART1 -->
# **前言**
國寫在升大學考試之國文科目中，與選擇題各佔一半分數之比重，惟我國之國文教學，向來重視古文閱讀、國學常識等，在教學大綱中以選擇題的答寫作為主要教學方向，許多學生對於國寫部分之掌握能力相對不足。因此，本組希望透過分析國寫情意題佳作詞頻，進一步洞察國寫在我國命題方向與佳作取材等寫作方式。從本組分析成果上來看，可以查知國寫範文有＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿等特色。

---

<!-- PART2 -->
# **研究方法**

<br>

## **1資料來源與介紹**
本組參酌大考中心每年提供之升大學考試國文作文佳作範本，收錄範圍為民國95年至110年學測與98年至110年指考，共計208篇。另外，由於大考中心係提供範文手寫影本，資料前處理較為耗時，為免壓縮後續資料處理時間，106至110年之學測與指考皆各收錄兩筆資料，並以完整結構之作文為主（新式國寫有部分簡答題）。

## **2資料前處理**
本組以手動繕打將資料輸入電腦轉換成txt.檔，統一資料格式（例如：分段換行、無空格、完整標點符號、刪除底線、標準化檔名）後，再匯入Ｒ、python進行後續資料分析。
處理後之資料格式範例顯示如下：

------------- --------------------------------------------
file type     .txt

file name     GSAT_107_1

file content  四季遞嬗，各有各的紛繁絢麗，也各有各的落寞寂寥。然而，一個季節的組成絕不只是天氣上的變化，還要有繚繞的氣味，刻骨的畫面，以及活在季節的人們的故事，最好還要有一支如韋瓦第四季的曲子，使季節如電影般，有開演、有落幕。\\n
              閉上眼睛，常常想起的畫面通常背景是。
------------- --------------------------------------------


---

<!-- PART3 -->
# **資料分析**
本組將成果報告分成以下4個部分，依序為**基礎結構分析**、**引用資料分析**、**語彙詞頻分析**與**相似性分析**，詳細說明如下：

<!-- 所有會用到的library -->
```{r, echo = FALSE}
library(jiebaR, jiebaRD)
library(stringr)
library(dplyr)
library(ggplot2)
library (tidyverse)
library(tidytext)
library (ggpubr)
library(lubridate)
library(readr)
library(ggrepel)
library(ggthemr)
```


<br>

## **1基礎結構分析**

### **1.1佳作文章長度**
```{r, echo = FALSE, message=FALSE}
#文章字數分佈圖
ggplot(data = df_unbreak) +
  geom_freqpoly(aes(x=article_len),size=1,color="#996666")+
  geom_vline(xintercept=549.75,color="grey",size=1)+
  geom_vline(xintercept=608,color="grey",size=0.5)+
  geom_vline(xintercept=677.25,color="grey",size=0.5)+
  labs(title="文章字數分佈密度圖",x="字數",y="篇數",caption ="*輔助線為四分位數（549.75,608,677.25)")+
  theme(text=element_text(size=12))
```

### **1.2文章句數**
```{r, echo = FALSE, message=FALSE}
#文章句數分佈圖
ggplot(data = df_unbreak) +
  geom_freqpoly(aes(x=sentence_num),size=1,color="#996666")+
  geom_vline(xintercept=12,color="grey",size=1)+
  geom_vline(xintercept=15,color="grey",size=0.5)+
  geom_vline(xintercept=19,color="grey",size=0.5)+
  scale_shape_manual(values=c(4,15))+
  labs(title="文章句數分佈密度圖",x="句數",y="篇數",caption = "*輔助線為四分位數（12,15,19)")+
  theme(text=element_text(size=12))
```

### **1.3段落分佈狀況**
```{r, echo = FALSE}
text <- list.files(path = "../data_set/", full.names = FALSE)## ÀÉ®×
paragraph_num <- vector("numeric", length = length(text))## number of paragraph
article_len <- vector("numeric", length = length(text))## length of article
sentence_num <- vector("numeric", length = length(text))## number of sentencce

post <- list()
for(i in seq_along(text)){
  post[[i]] <- readLines(paste0("../data_set/", text[i]), encoding = "UTF-8", warn = FALSE)
  paragraph_num[i] <- length(post[[i]])
  a <- post[[i]][1]
  for(j in 2 : length(post[[i]])){
    a <- paste0(a, post[[i]][j])
  }
  post[[i]] <- a
  
}

post <- unlist(post)## turn to vector
for(i in seq_along(post)){ 
  sentence_num[i] <- length(unlist(strsplit(post[i], split = "[。！？]", fixed = FALSE)))
}


seg <- worker()
content <- vector("character", length(post))
for(i in seq_along(post)){
  segged <- segment(post[i], seg)
  article_len[i] <- nchar(paste(segged, collapse = ""))
  content[i] <- paste(segged, collapse = "\u3000")
}

df_break <- tibble::tibble(
  id = text,
  content = content
)

df_unbreak <- tibble::tibble(
  id = text,
  par_num = paragraph_num,
  sentence_num = sentence_num,
  article_len = article_len,
  content = post
)

ggthemr('dust')

#文章段落數分佈圖
ggplot(data = df_unbreak) +
  geom_bar(aes(x=par_num),size=1,color="#996666")+
  labs(title="文章段落數分佈密度圖", x="段落數", y="篇數")+
  theme(text=element_text(size=12))
```


<br>

## **2引用資料分析**
### **2.1名人偉人**
<div class = "row">

<div class = "col-md-6">
(分析結果描述)
</div>

<div class = "col-md-6">
```{r, echo = FALSE, out.width='100%'}
## 讀檔
id <- list.files("../data_set/") # 文章的檔名
contents <- vector("character", length(id)) # 創一個長度與文章數相同的vector

# 將文章內容讀進來，並放入contents這個變數中(不分段)
for (i in seq_along(id)){
  path <- paste0("../data_set/", id[i])
  contents[i] <- paste0(readLines(path, encoding = "UTF-8", warn=FALSE), collapse = "")
}

# 把檔名(id)和文章內容(contents)放進docs_df這個dataFrame中
docs_df <- tibble::tibble(
  id = id,
  content = contents
)

## 製圖

# 將放有偉人、名人名字的txt檔讀進來，存進names這個變數中
names <- readLines("../names.txt", encoding = "UTF-8", warn=FALSE)
names <- paste0(names, "、", collapse = "")
names <- unique(strsplit(names, split = "、")[[1]]) 

# 統計各篇文章出現幾個偉人、名人(若重複提到只計算一次)，儲存結果於docs_df的name_times欄位中
docs_df$name_times <- unlist(lapply(contents, function(x){sum(str_detect(x, names))}))

#製圖
ggthemr('dust')

docs_df1 <-docs_df%>% group_by(name_times)%>%summarise(count =n(),percent =n()/208)
ggplot(docs_df1, aes(x=reorder(name_times,-count),y=count)) +
       geom_bar(width = 0.8, stat = "identity") +
   
#加上標題
labs(title="大考佳作出現名人次數分佈狀況",x="出現次數",y="篇數")+

#修改標籤名字
theme(text=element_text(size=12))+
geom_text(aes(label = count,y = -10),colour="black",size=4)+
geom_text(aes(label = paste0(sprintf("%1.1f", percent*100),"%"), y=(count+10)),size=6,position =position_dodge(0.2),colour="brown")
```
</div>
</div>

<br>

<div class = "row">

<div class = "col-md-4">
(分析結果描述)
</div>

<div class = "col-md-8">
```{r, echo = FALSE, out.width='100%'}
# 哪個名人、偉人出現在最多篇文章中
# 名字計數器(傳入某名字，函數會回傳提到此名字的文章有幾篇)
count_times <- function(name){
  count <- 0 
  for(i in seq_along(contents)){
    if (str_detect(contents[[i]], name) == TRUE){
      count <- count + 1
    }
  }
  return (count)
}

# 計算每個名字各出現在幾篇文章中
article_found_Q <- unlist(lapply(names, count_times))

# 出現某名人的文章是哪幾篇
find_article <- function(name){
  article_vec <- c()
  for (i in seq_along(contents)){
    if (str_detect(contents[[i]], name) == TRUE){
        article_vec <- c(article_vec, id[[i]])
    }
  }
  return(paste0(article_vec, " ", collapse = ""))
}
article_found <- unlist(lapply(names, find_article))

# 將名字與篇數放入names_df中
names_df <- tibble::tibble(
  name = names,
  article_found_Q = article_found_Q,
  article_found = article_found
  )

## 圖
ggthemr('dust')

#製圖
names_df1 <- names_df %>%
  filter(article_found_Q >= 4)

ggplot(names_df1, aes(x = reorder(name, -article_found_Q), y = article_found_Q)) +
  geom_bar(width = 0.8, stat = "identity") +
  #加上標題
  labs(title = "名人出現次數排行", x = "人名", y = "篇數") +
  #修改標籤名字
  theme(text = element_text(size = 12), axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  geom_text(aes(label = article_found_Q, y=( article_found_Q+1)),size=6,position =position_dodge(0.2),colour="brown")  
```
</div>
</div>

### **2.2佳句**
**佳句整理**
```{r, echo=FALSE, attr.output='style="max-height: 300px;"'}
good_sent <- c()
good_sent_id <- c()

# 含冒號
for (i in seq_along(id)){
  sentences <- unique(str_match_all(contents[[i]], "：(\\w|，){5,}(。|？|！)")[[1]][,1])
  good_sent <- c(good_sent, sentences)
  good_sent_id <- c(good_sent_id, rep(id[i], length(sentences)))
}

# 含引號
for (i in seq_along(id)){
  sentences <- unique(str_match_all(contents[[i]], "「(\\w|，|。|！|；|、){5,}?」")[[1]][,1])
  good_sent <- c(good_sent, sentences)
  good_sent_id <- c(good_sent_id, rep(id[i], length(sentences)))
}

# 拿掉冒號和上下引號
good_sent <- unlist(lapply(good_sent, function(x){str_replace_all(x, "[：「」]", "")}))


# 人工移除不合適結果
good_sent <- good_sent[-c(5, 6, 7, 8, 9, 10, 11, 13, 18, 19, 20, 22, 24, 25, 26, 27, 28, 32, 35, 40, 43, 44, 58, 61, 73, 74, 75, 76, 83, 85, 91, 95, 96, 97, 102, 103, 104, 105, 109, 115, 118, 119, 122, 124, 126, 128, 132, 135, 137, 138, 143, 146, 148, 150, 159, 160, 163, 169, 171, 174, 175, 176, 189, 193, 194, 197, 203, 208, 213, 216, 219, 221, 223, 232, 235, 242, 243, 245, 258, 259, 268, 273, 274, 275, 276, 278, 288, 304, 306, 309, 312, 314, 315, 316, 323, 324, 329, 330, 336, 345, 346, 347)]

good_sent_id <- good_sent_id[-c(5, 6, 7, 8, 9, 10, 11, 13, 18, 19, 20, 22, 24, 25, 26, 27, 28, 32, 35, 40, 43, 44, 58, 61, 73, 74, 75, 76, 83, 85, 91, 95, 96, 97, 102, 103, 104, 105, 109, 115, 118, 119, 122, 124, 126, 128, 132, 135, 137, 138, 143, 146, 148, 150, 159, 160, 163, 169, 171, 174, 175, 176, 189, 193, 194, 197, 203, 208, 213, 216, 219, 221, 223, 232, 235, 242, 243, 245, 258, 259, 268, 273, 274, 275, 276, 278, 288, 304, 306, 309, 312, 314, 315, 316, 323, 324, 329, 330, 336, 345, 346, 347)]

# 佳句
good_sent

```

**重複出現之佳句**
```{r, echo=FALSE}
# 去除所有標點符號
good_sent_clean <- unlist(lapply(good_sent, function(x){str_replace_all(x, "[，。！、]", "")}))

# 計算相似度
simhasher = worker("simhash", topn = 10)
for(i in 1:(length(good_sent)-1)){
  for(j in((i+1):length(good_sent))){
    sim <- distance(good_sent_clean[i], good_sent_clean[j], simhasher)$distance
    if (sim < 12){
      if (!(good_sent_id[i] == good_sent_id[j])){
        cat(good_sent_id[i], good_sent[i], "\n")
        cat(good_sent_id[j], good_sent[j], "\n")
        cat("\n")
      }
    }
  } 
}
```

<br>

## **3語彙詞頻分析**
### **3.1中文語彙詞頻**
### **3.2冷僻字使用佔比**

<br>

## **4相似性分析**
### **4.1首段與尾段落相似性**
### **4.2輸入新文章分析器**

---

# **小結**

---

# **反思**
